"""Final Project ML

Shriji Shah (100665031)
Zachary Silver (100752283)
INFR 3700 Final Assignment

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sr2VIOGRCJzmlmsQqE25BQoJrt2i3C-w
"""

# Imports
import seaborn as sns
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from tensorflow import keras
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import operator


data = pd.read_csv(
    'https://raw.githubusercontent.com/OzymandiasThe2/machine_learning_final_project/main/anime.csv')

# understanding the data
data.head()

# will show all the integer object columns --> drop this
data.describe()

# Shape the data
data.shape

# check for columns and rows

data.columns


'''
################## DEBUG Dataframes

# checking for unique values

data.nunique()
# print("Unique Types of anime = ", data['Type'].unique())
# print("\nUnique Genres of anime = ", data["Genres"].unique())
# print("\nUnique Rating of anime = ", data["Rating"].unique())
'''
# cleaning up the data
## Confirm if any null columns
data.isnull().sum()

# dropping values

dataSet = data.drop(columns=["MAL_ID", "Genres", "English name", "Japanese name", "Aired", "Premiered",
                             "Producers", "Licensors", "Duration", "Members", "Favorites", "Watching",
                             "Completed", "On-Hold", "Dropped", "Plan to Watch",
                             "Score-10", "Score-9", "Score-8", "Score-7", "Score-6", "Score-5",
                             "Score-4", "Score-3", "Score-2", "Score-1"])

dataSet.head()

# shows only Popularity as ints, therefore we have to turn the rest of the int columns into actula intgers
dataSet.describe()

# drop the "unknown" values from the column
drop_score_unknown = dataSet[(dataSet["Score"] == "Unknown")].index
dataSet.drop(drop_score_unknown, inplace=True)

# convert the column from string to integers
dataSet["Score"] = dataSet["Score"].astype(str).astype(float)

# drop the "unknown" values from the column
drop_ranked_unknown = dataSet[(dataSet["Ranked"] == "Unknown")].index
dataSet.drop(drop_ranked_unknown, inplace=True)

# convert the column from string to integers
dataSet["Ranked"] = dataSet["Ranked"].astype(str).astype(float)

# Make sure that are columns are presented right data
dataSet.describe()

# Since the data here is vastly large with 50k entires, we'll compare data points of the top 100 titles and the
# bottom 100 titles


# TOP 100 from the top
top2000 = dataSet.nlargest(2000, ['Score'])

print("\nUnique Source of anime = ", dataSet["Source"].unique())
print("\nUnique Rating of anime = ", dataSet["Rating"].unique())
print("\nUnique Rating of anime = ", data["Rating"].unique())


def plots():
    sns.pairplot(dataSet)

    sns.relplot(x="Popularity", y="Score", hue="Rating", data=dataSet)
    plt.gca().invert_xaxis()

    test1 = sns.relplot(x="Ranked", y="Popularity", hue="Type", data=dataSet, aspect=1.5, kind="line", )

    # inverses the y axis and x axis
    plt.gca().invert_yaxis()
    plt.gca().invert_xaxis()

    sns.relplot(x="Popularity", y="Score", hue="Source", data=dataSet, aspect=1.5)
    plt.gca().invert_xaxis()

    sns.relplot(x="Score", y="Popularity", hue="Rating", data=top2000, aspect=2)
    plt.gca().invert_yaxis()

    sns.lmplot(y="Ranked", x="Score", ci=None, data=top2000)
    plt.gca().invert_xaxis()


plots()

"""# Algorithms

## First Algorithm - Polynomial Regression
"""

#Store each algorithm in a function
def polynomial():
    droppedSet = top2000.copy()

    poorModel = linear_model.LinearRegression()

    # Train linear model
    x = np.c_[droppedSet.iloc[:, 7]]
    y = np.c_[droppedSet.iloc[:, 1]]
    poorModel.fit(x, y)

    # Plot points for graph
    plt.scatter(x, y, color='k')
    plt.xlabel("Ranked")
    plt.ylabel("Score")

    polynomial_features = PolynomialFeatures(degree=2)
    x_poly = polynomial_features.fit_transform(x)

    model = LinearRegression()
    model.fit(x_poly, y)
    y_poly_pred = model.predict(x_poly)

    rmse = np.sqrt(mean_squared_error(y, y_poly_pred))
    r2 = r2_score(y, y_poly_pred)
    print("RMSE = ", rmse)
    print("R2 = ", r2)

    plt.scatter(x, y, s=10)
    # sort the values of x before line plot
    sort_axis = operator.itemgetter(0)
    sorted_zip = sorted(zip(x, y_poly_pred), key=sort_axis)
    x, y_poly_pred = zip(*sorted_zip)
    plt.gca().invert_xaxis()

    plt.plot(x, y_poly_pred, color='m')
    plt.show()


"""## Second Algorithm - Ridge Lasso"""

#Lasso function
def lasso():
    # Setting the training and testing data
    trainData, testData = train_test_split(top2000, test_size=0.5)

    # Create poor linear model
    poorModel = linear_model.LinearRegression()

    # Train linear model
    x = np.c_[top2000.iloc[:, 7]]
    y = np.c_[top2000.iloc[:, 1]]
    poorModel.fit(x, y)

    # Plot points for graph
    plt.scatter(x, y, color='k')
    plt.xlabel("Ranked")
    plt.ylabel("Score")
    plt.gca().invert_xaxis()

    # Plotting the poor model
    model_features = PolynomialFeatures(degree=2, include_bias=False)
    # Setting the parameters for the axis
    plt.yticks(np.arange(0, 50, 3))
    plt.xticks(np.arange(0, 26, 2.5))

    # Fit graph to x axis data
    xPoly = model_features.fit_transform(x)
    # Graph as a linear regression
    linRegression = linear_model.LinearRegression()
    linRegression.fit(xPoly, y)

    # Adapt data to linear regression line
    newX = np.linspace(7, 25, 100).reshape(100, 1)
    newPolyx = model_features.transform(newX)
    newY = linRegression.predict(newPolyx)
    plt.plot(newX, newY, "m-", linewidth=2, label="Linear Regression")

    # Calculate poor MSE
    poorPredict = linRegression.predict(xPoly)
    poorMSE = mean_squared_error(y, poorPredict)

    # Create the legend and title of the graph
    plt.legend()
    plt.title("Model: Score vs. Ranked")

    # Training the better model
    ridge_reg = Ridge(alpha=1, solver="sag", random_state=42)
    ridge_reg.fit(x, y)
    lasso_reg = Lasso(alpha=0.1, random_state=42)
    lasso_reg.fit(x, y)
    lassoY = ridge_reg.predict(newX)

    # Calculate good MSE
    goodPredict = ridge_reg.predict(x)
    goodMSE = mean_squared_error(y, goodPredict)

    # Plotting the lasso line
    plt.plot(newX, lassoY, "c-", linewidth=2, label="Lasso")
    plt.legend()
    plt.gca().invert_xaxis()

    # Print the MSEs
    print(poorMSE)
    print(goodMSE)


"""## Third Algorithm - DNN """
sal_data = top2000
sal_data['Score'].value_counts()
# sal_data['Ranked'].value_counts()

"""We will predict the hobby, which is categorical, so we need to convert it to numerical values

Next, we apply our usual split but this time we do a 50/50 split to match the size of the training set with the 
validation set """

top2000.head()

train, test = train_test_split(sal_data, test_size = 0.5)  # , random_state=11)


def create_ids(df):
    ids = []
    storage = {}

    max_id = 0

    for item in df:
        if item in storage:
            ids.append(storage[item])
        else:
            storage[item] = max_id
            max_id += 1
            ids.append(storage[item])
    return storage, ids


# KEEP 'Type',"Studios","Source","Rating" but converting into integers through the create_id function

train_data = train.drop(['Name', "Episodes"], axis=1)

train_data['Studios'] = create_ids(train['Studios'])[1]
train_data['Type'] = create_ids(train['Type'])[1]
train_data['Source'] = create_ids(train['Source'])[1]
train_data['Rating'] = create_ids(train['Rating'])[1]
train_labels = train_data.iloc[:, 1]

test_data = test.drop(['Name', "Episodes"], axis=1)
test_data['Studios'] = create_ids(test['Studios'])[1]
test_data['Type'] = create_ids(test['Type'])[1]
test_data['Source'] = create_ids(test['Source'])[1]
test_data['Rating'] = create_ids(test['Rating'])[1]

test_labels = test_data.iloc[:, 1]
# For multiclass classification, we will use 'Hobby' as the target
train_labels_mc = train['Score']
test_labels_mc = test['Score']

train_data

test_data

test_labels

train_data

train_labels

plt.figure(figsize=(8,8))
sns.heatmap(test_data.corr(), annot=True, cmap="coolwarm")


"""Scale as usual:"""

scaler = StandardScaler()
train_data = scaler.fit_transform(train_data)
test_data = scaler.transform(train_data)

"""Let's see the versions available in Colab"""

print('tf version:', tf.__version__, 'keras version:', keras.__version__)

"""Available activations in Keras:"""

# [name for name in dir(keras.activations) if not name.startswith("_")]

"""Then we create a fully connected neural network"""

model = keras.models.Sequential([

    # keras.layers.Flatten(input_shape=[7]),
    keras.layers.BatchNormalization(input_shape=[7]),
    keras.layers.Dense(20, activation="relu", kernel_initializer="HeNormal"),
    keras.layers.Dense(20, activation="elu"),
    keras.layers.Dropout(rate=0.5),
    keras.layers.Dense(20, activation="softplus")
])

"""Available initializers in Keras:"""

# [name for name in dir(keras.initializers) if not name.startswith("_")]

"""Visualize the model:"""

model.build()
model.summary()

keras.utils.plot_model(model, "score.png", show_shapes=True,
                       rankdir='LR')  # Try removing rankdir = 'LR'

"""And then establish loss, optimizer, and metric, but modify parameters for better performance of deep nets"""

opt = keras.optimizers.RMSprop(
    learning_rate=0.1,
    rho=0.09,
    momentum=0.001,
    decay=1e-6,
    epsilon=1e-07
)
# opt = keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07)
model.compile(loss="mse",
              optimizer=opt,
              metrics=["mae"])

train

"""We will use the history object to create a graph"""

tf.convert_to_tensor(train_data)

#Fit to model
history = model.fit(
    tf.convert_to_tensor(train_data),
    train_labels_mc, batch_size=10,

    epochs=100,
    validation_data=(test_data, test_labels_mc),
    verbose=1)  # Turn verbose=1 (default) for printing epochs

#Generate and display the model
pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 5)

plt.show()

pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1)

plt.show()

# Best Performing Algorithm
history = model.fit(
    tf.convert_to_tensor(train_data),
    train_labels_mc, batch_size=10,

    epochs=100,
    validation_data=(test_data, test_labels_mc),
    verbose=1)  # Turn verbose=1 (default) for printing epochs

plt.plot(history.history['val_mae'])
plt.plot(history.history['mae'])
# plt.plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1)

plt.title('mae Comparisons')
# plt.grid(True)
# plt.legend([mae], ['mae'])

#Display and show all the plots for the end results
predict = model.predict(test_data)

print(predict)

y = test["Score"]
x = test["Type"]
plt.figure(figsize=(18, 5))

plt.bar(x, y)
plt.title('Anime Type by Score')
plt.xlabel('Type')
plt.ylabel('Score')

plt.grid(True)
plt.gca().set_ylim(0, 10)
plt.show()


x = test["Type"]
y = test["Popularity"]
plt.figure(figsize=(20, 5))

plt.bar(x, y)
plt.title('Anime Type by Popularity')
plt.xlabel('Type')
plt.ylabel('Popularity')

plt.grid(True)
plt.gca().set_ylim(0, 12000)
plt.show()

y = test["Score"]
x = test["Rating"]
plt.figure(figsize=(18, 5))

plt.bar(x, y)
plt.title('Anime Rating by Score')
plt.xlabel('Rating')
plt.ylabel('Score')

plt.grid(True)
plt.gca().set_ylim(0, 10)
plt.show()

x = test["Source"]
y = test["Score"]
plt.figure(figsize=(20, 5))

plt.bar(x, y)
plt.title('Anime Source by Score')
plt.ylabel('Score')
plt.xlabel('Source')

plt.grid(True)
plt.gca().set_ylim(0, 9.5)
plt.show()

lasso()
polynomial()
